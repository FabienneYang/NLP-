{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=[]\n",
    "paths =[r'spam_data/spam', r'spam_data/easy_ham', r'spam_data/hard_ham'] \n",
    "for path in paths:\n",
    "    for fn in glob.glob(path+\"/*\"):\n",
    "        if \"ham\" not in fn:\n",
    "            is_spam = 1\n",
    "        else:\n",
    "            is_spam = 0\n",
    "        #codecs.open可以避開錯誤，用errors='ignore'\n",
    "        with codecs.open(fn,encoding='utf-8', errors='ignore') as file:\n",
    "            for line in file:\n",
    "                #這個line的開頭為Subject:\n",
    "                if line.startswith(\"Subject:\"):\n",
    "                    subject=re.sub(r\"^Subject:\",\"\",line).strip()\n",
    "                    all_data.append([subject,is_spam])\n",
    "all_data = np.array(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Life Insurance - Why Pay More?', '1'],\n",
       "       ['[ILUG] Guaranteed to lose 10-12 lbs in 30 days 10.206', '1'],\n",
       "       ['Guaranteed to lose 10-12 lbs in 30 days                          11.150',\n",
       "        '1'],\n",
       "       ...,\n",
       "       ['NEWS: GNU/DEVELOPMENT... intl orgns take a close look at GNU/Linux',\n",
       "        '0'],\n",
       "       ['Attn programmers: support offered [FLOSS-Sarai Initiative]',\n",
       "        '0'],\n",
       "       ['(SPAM? 08.00) lists.sourceforge.net mailing list memberships reminder',\n",
       "        '0']], dtype='<U109')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_data[:,0]\n",
    "Y = all_data[:,1].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Examples : \n",
      "['Life Insurance - Why Pay More?'\n",
      " '[ILUG] Guaranteed to lose 10-12 lbs in 30 days 10.206'\n",
      " 'Guaranteed to lose 10-12 lbs in 30 days                          11.150'\n",
      " 'Re: Fw: User Name & Password to Membership To 5 Sites zzzz@example.com pviqg'\n",
      " '[ILUG-Social] re: Guaranteed to lose 10-12 lbs in 30 days 10.148']\n"
     ]
    }
   ],
   "source": [
    "print('Training Data Examples : \\n{}'.format(X[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling Data Examples : \n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print('Labeling Data Examples : \\n{}'.format(Y[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fabienne2.Yang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#文字預處理\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "## 創建Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"將pos_tag結果mapping到lemmatizer中pos的格式\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def clean_content(X):\n",
    "    # remove non-alphabet characters\n",
    "    X_clean = [re.sub('[^a-zA-Z]',' ', x).lower() for x in X]\n",
    "    # tokenize\n",
    "    X_word_tokenize = [nltk.word_tokenize(x) for x in X_clean]\n",
    "    # stopwords_lemmatizer\n",
    "    X_stopwords_lemmatizer = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for content in X_word_tokenize:\n",
    "        content_clean = []\n",
    "        for word in content:\n",
    "            if word not in stop_words:\n",
    "                word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "                content_clean.append(word)\n",
    "        X_stopwords_lemmatizer.append(content_clean)\n",
    "    \n",
    "    X_output = [' '.join(x) for x in X_stopwords_lemmatizer]\n",
    "    \n",
    "    return X_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_content(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#max_features是要建造幾個column，會按造字出現的頻率高低去篩選，1500並沒有特別含義，大家可以自己嘗試不同數值或不加入限制\n",
    "cv=CountVectorizer(max_features = 1500)\n",
    "X=cv.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3423, 1500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 有 3423個樣本，每個樣本用1500維表示\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#將資料拆成 train/test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# random_state 是為了得到相同的結果，平時可以移除\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset Accuracy: 0.8794740686632578\n"
     ]
    }
   ],
   "source": [
    "#測試 train/testset的 Accuracy\n",
    "print('Trainset Accuracy: {}'.format(classifier.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset Accuracy: 0.8248175182481752\n"
     ]
    }
   ],
   "source": [
    "print('Testset Accuracy: {}'.format(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#獲得 testset 上的結果\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[529  58]\n",
      " [ 62  36]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8248175182481752"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "設置K值:5\n",
      "Average Accuracy: 0.8659864709499747\n",
      "Accuracy STD: 0.02356923199911115\n",
      "設置K值:10\n",
      "Average Accuracy: 0.8656000981986385\n",
      "Accuracy STD: 0.005491430871408997\n",
      "設置K值:25\n",
      "Average Accuracy: 0.8524506102170335\n",
      "Accuracy STD: 0.00138641752327577\n",
      "設置K值:50\n",
      "Average Accuracy: 0.8520856467133839\n",
      "Accuracy STD: 0.0014068907077920568\n",
      "設置K值:100\n",
      "Average Accuracy: 0.8520856467133839\n",
      "Accuracy STD: 0.0014068907077920568\n"
     ]
    }
   ],
   "source": [
    "#運用K-fold尋找適合K值\n",
    "# Applying k-Fold Cross Validation\n",
    "#n-jobs=-1，是指cpu全開\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "n_neighbors  = [5, 10, 25, 50, 100] ## 可自行嘗試不同K值\n",
    "for k in n_neighbors:\n",
    "    classifier = KNeighborsClassifier(n_neighbors = k, metric = 'minkowski', p = 2)\n",
    "    # cv = 10 代表切成10等分\n",
    "    accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10,n_jobs=-1)\n",
    "    \n",
    "    print('設置K值:{}'.format(k))\n",
    "    print('Average Accuracy: {}'.format(accuracies.mean()))\n",
    "    print('Accuracy STD: {}'.format(accuracies.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
