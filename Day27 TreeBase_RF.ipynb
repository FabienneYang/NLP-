{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用Gini計算訊息增益\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "# 分割測試集與訓練集  \n",
    "def train_test_split(df, test_size=0.1):\n",
    "    \n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(df))\n",
    "\n",
    "    #以隨機的方式取的測試集資料點的index\n",
    "    indices = list(df.index)\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "\n",
    "    #分割測試集與訓練集\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# 檢查資料是否都為同一類別\n",
    "def check_purity(data):\n",
    "    '''Function to check if input data all belong to the same class\n",
    "    Parameter\n",
    "    ---------\n",
    "    data: list\n",
    "        Input data\n",
    "    '''\n",
    "    #取的資料的label訊息\n",
    "    labels = data[:, -1]\n",
    "    \n",
    "    #檢查是否所有的label都為同一種\n",
    "    unique_classes = np.unique(labels)\n",
    "    \n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# 根據給定的資料，取得每個特徵(feature)可能做為樹型模型分割節點的值\n",
    "# 可能作為分割節點得值即為每個特徵的獨特值(unique value)\n",
    "def get_potential_splits(data, random_features=None):\n",
    "    '''Function to get all potential split value for tree base model\n",
    "    Parameter\n",
    "    ---------\n",
    "    data: list\n",
    "        Input data\n",
    "    '''\n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    column_indices = list(range(n_columns - 1)) #此處的-1是為了扣掉label的欄位\n",
    "    \n",
    "    if random_features and random_features <= len(column_indices):\n",
    "        #隨機選取特徵進行訓練\n",
    "        column_indices = random.sample(population=column_indices, k=random_features)\n",
    "    \n",
    "    for column_index in column_indices:    \n",
    "        \n",
    "        #根據欄位取的特徵的獨特值(unique values)\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        \n",
    "        #將取得的可能分割值除存在potential_split的字典中(key=特徵欄位的index, value:此特徵可能的分割值)\n",
    "        potential_splits[column_index] = unique_values\n",
    "    \n",
    "    return potential_splits\n",
    "\n",
    "\n",
    "#由給定的輸入DataFrame給個特徵值的型態(數值型特徵或類別型特徵)\n",
    "def determine_type_of_feature(df):\n",
    "    '''Function to get features types\n",
    "    Parameter\n",
    "    ---------\n",
    "    df: pd.DataFrame\n",
    "        Input raw pd.DataFrame data\n",
    "    '''\n",
    "    \n",
    "    feature_types = []\n",
    "    \n",
    "    #若特徵的獨特值個數較少，及當作類別型特徵資料(若為數值型，獨特值個數應該會很多)\n",
    "    #此處簡易的將判斷方法設為資料個數的1/3次方，此值可以自行修改選較為適合的個數\n",
    "    n_unique_values_treshold = int(len(df)**(1/3))\n",
    "    \n",
    "    for feature in df.columns:\n",
    "        if feature != \"label\":\n",
    "            unique_values = df[feature].unique()\n",
    "            rep_value = unique_values[0] #選出一個值做此特徵的代表\n",
    "\n",
    "            if (isinstance(rep_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "    \n",
    "    return feature_types\n",
    "\n",
    "\n",
    "# 根據給定的資料、欲採用特徵欄位指標(index)與欲採用的分割值，來取的分割節點分割後的左節點資料與右節點資料\n",
    "def split_data(data, split_column, split_value):\n",
    "    '''Function to splitted left and right nodes\n",
    "    Parameter\n",
    "    ---------\n",
    "    data: list\n",
    "        Input data\n",
    "    split_column: int\n",
    "        index for feature column\n",
    "    split_value: float or int or string\n",
    "        value to be used as split benchmark\n",
    "    '''\n",
    "    \n",
    "    #取得用來分割的特徵欄位\n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    #依據欄位值的型態(數值型特徵或類別型特徵)來進行節點分割\n",
    "    type_of_feature = FEATURE_TYPES[split_column]\n",
    "    \n",
    "    if type_of_feature == \"continuous\":\n",
    "        #數值型特徵分割\n",
    "        data_left = data[split_column_values <= split_value]\n",
    "        data_right = data[split_column_values >  split_value]\n",
    "    else:\n",
    "        #類別型特徵分割\n",
    "        data_left = data[split_column_values == split_value]\n",
    "        data_right = data[split_column_values != split_value]\n",
    "    \n",
    "    return data_left, data_right\n",
    "\n",
    "\n",
    "# 根據給定的資料與任務類型(回歸或分類)來產生終端節點\n",
    "def create_leaf(data, task_type):\n",
    "    '''Function to create leaf node\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: list\n",
    "        Input data\n",
    "    task_type: str\n",
    "        indicate the type of tree (regression or classification)\n",
    "    '''\n",
    "    \n",
    "    #取的資料的label欄位\n",
    "    label_column = data[:, -1]\n",
    "    \n",
    "    if task_type == \"regression\":\n",
    "        #回歸任務\n",
    "        leaf = np.mean(label_column)\n",
    "    else:\n",
    "        #分類任務\n",
    "        #取得所有輸入資料的獨立類別與其個數\n",
    "        unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "        \n",
    "        #以個數最多的類別，作為此節點的輸出類別\n",
    "        index = counts_unique_classes.argmax()\n",
    "        leaf = unique_classes[index]\n",
    "    \n",
    "    return leaf\n",
    "\n",
    "#計算資料的熵(Entropy)\n",
    "def calculate_entropy(data):\n",
    "    \n",
    "    #取的資料的label訊息\n",
    "    label_column = data[:, -1]\n",
    "    \n",
    "    #取得所有輸入資料的獨立類別與其個數\n",
    "    _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "    #計算機率\n",
    "    probabilities = counts / counts.sum()\n",
    "    \n",
    "    #計算entropy\n",
    "    entropy = sum(probabilities * -np.log2(probabilities))\n",
    "     \n",
    "    return entropy\n",
    "\n",
    "\n",
    "#取得左節點與右節點訊息合\n",
    "def calculate_overall_metric(data_below, data_above, metric_function):\n",
    "    \n",
    "    n = len(data_below) + len(data_above)\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_metric =  (p_data_below * metric_function(data_below) \n",
    "                     + p_data_above * metric_function(data_above))\n",
    "    \n",
    "    return overall_metric\n",
    "\n",
    "\n",
    "#以迴圈的方式計算所有可能分割值的訊息增益，取的最佳的分割特徵與值(訊息增益最大)\n",
    "def determine_best_split(data, potential_splits, metric_function, task_type='classification'):\n",
    "    \n",
    "    #紀錄是否為樹的第一層(第一次回圈)\n",
    "    first_iteration = True\n",
    "    \n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            \n",
    "            #根據給定的特徵與分割值分割資料為左節點、右節點\n",
    "            data_left, data_right = split_data(data, split_column=column_index, split_value=value)\n",
    "            \n",
    "            #判斷是回歸樹亦或分類樹\n",
    "            if task_type == \"regression\":\n",
    "                #回歸樹\n",
    "                current_overall_metric = calculate_overall_metric(data_left, data_right, metric_function=metric_function)\n",
    "            else:\n",
    "                #分類樹\n",
    "                current_overall_metric = calculate_overall_metric(data_left, data_right, metric_function=metric_function)\n",
    "\n",
    "            if first_iteration or current_overall_metric <= best_overall_metric:\n",
    "                first_iteration = False\n",
    "                \n",
    "                best_overall_metric = current_overall_metric\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value\n",
    "\n",
    "\n",
    "class decision_tree():\n",
    "    '''Decision Tree model\n",
    "    Parameters\n",
    "    -----------\n",
    "    metric_function: function\n",
    "        the metric function used to calculate information gain\n",
    "    task_type: str\n",
    "        indicate the type of tree (regression or classification)\n",
    "    counter: int\n",
    "        counter for recording number of splits\n",
    "    min_samples: int\n",
    "        minimum number of samples for a node to be able to split\n",
    "    max_depth: int\n",
    "        Maximum depth for the decision tree\n",
    "    '''\n",
    "    def __init__(self, metric_function, task_type='classification', counter=0, min_samples=2, max_depth=5, random_features=None):\n",
    "        \n",
    "        self.metric_function = metric_function\n",
    "        self.task_type = task_type\n",
    "        self.counter = counter\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.random_features = random_features\n",
    "    \n",
    "    def fit(self, df):\n",
    "        '''\n",
    "        df: pd.DataFrame\n",
    "            input raw DataFrame data\n",
    "        '''\n",
    "        # 資料準備\n",
    "        if self.counter == 0:\n",
    "            #若為第一次分割，取出資料特徵的欄位與其對應的型態\n",
    "            global COLUMN_HEADERS, FEATURE_TYPES\n",
    "\n",
    "            #取得資料特徵欄位\n",
    "            COLUMN_HEADERS = df.columns\n",
    "            #取的特徵型態\n",
    "            FEATURE_TYPES = determine_type_of_feature(df)\n",
    "            #取得資料特徵值\n",
    "            data = df.values\n",
    "        else:\n",
    "            #取得資料特徵值\n",
    "            data = df           \n",
    "\n",
    "        # 終端節點處理(leaf)\n",
    "        # 若資料都屬於同一種類別、資料個數小於最小可分割個數、樹的深度大於最大深度，節點即屬於終端節點(leaf)\n",
    "        if (check_purity(data)) or (len(data) < self.min_samples) or (self.counter == self.max_depth):\n",
    "            leaf = create_leaf(data, self.task_type)\n",
    "            return leaf\n",
    "\n",
    "        # 分割節點\n",
    "        else:    \n",
    "            self.counter += 1\n",
    "\n",
    "            # 節點分割的左節點與右節點\n",
    "            potential_splits = get_potential_splits(data, self.random_features)\n",
    "            split_column, split_value = determine_best_split(data, potential_splits,\n",
    "                                                             self.metric_function, self.task_type)\n",
    "            data_left, data_right = split_data(data, split_column, split_value)\n",
    "\n",
    "            # 若分割後的左節點或右節點sample個數為零(代表母節點即無法在分割)\n",
    "            if len(data_left) == 0 or len(data_right) == 0:\n",
    "                # 取出此節點\n",
    "                leaf = create_leaf(data, self.task_type)\n",
    "                return leaf\n",
    "\n",
    "            # 取得分割節點的分割依據(特徵與分切值)\n",
    "            feature_name = COLUMN_HEADERS[split_column]\n",
    "            type_of_feature = FEATURE_TYPES[split_column]\n",
    "\n",
    "            if type_of_feature == \"continuous\":\n",
    "                #連續型數值\n",
    "                question = \"{} <= {}\".format(feature_name, split_value)\n",
    "            else:\n",
    "                #類別型數值\n",
    "                question = \"{} = {}\".format(feature_name, split_value)\n",
    "\n",
    "            # 建構子樹(sub-tree)\n",
    "            sub_tree = {question: []}\n",
    "\n",
    "            # 已遞迴的方式取建構完整決策樹    \n",
    "            yes_answer = self.fit(data_left)\n",
    "            no_answer = self.fit(data_right)\n",
    "\n",
    "            #若左節點與右節點分割的結果相同，則此節點及不需再進行分割\n",
    "            #此情形會發生在此節點資料個數小於min_samples或樹深度大於max_depth\n",
    "            if yes_answer == no_answer:\n",
    "                sub_tree = yes_answer\n",
    "            else:\n",
    "                sub_tree[question].append(yes_answer)\n",
    "                sub_tree[question].append(no_answer)\n",
    "            \n",
    "            self.sub_tree = sub_tree\n",
    "            \n",
    "            return self.sub_tree\n",
    "        \n",
    "    def pred(self, example, tree):\n",
    "        # 使用訓練好的決策樹進行預測\n",
    "        \n",
    "        #取得分割節點(由上到下)\n",
    "        question = list(tree.keys())[0]\n",
    "        feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "        #以節點分割問題分類資料\n",
    "        if comparison_operator == \"<=\":\n",
    "            #數值型資料\n",
    "            if example[feature_name] <= float(value):\n",
    "                answer = tree[question][0]\n",
    "            else:\n",
    "                answer = tree[question][1]\n",
    "        else:\n",
    "            #類別型資料\n",
    "            if str(example[feature_name]) == value:\n",
    "                answer = tree[question][0]\n",
    "            else:\n",
    "                answer = tree[question][1]\n",
    "        \n",
    "        # 若分類完成，返回分類結果\n",
    "        if not isinstance(answer, dict):\n",
    "            return answer\n",
    "        else:\n",
    "            #繼續往下分類\n",
    "            residual_tree = answer\n",
    "            return self.pred(example, residual_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>diameter</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Green</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Red</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Grape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Grape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>3.3</td>\n",
       "      <td>Lemon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    color  diameter  label\n",
       "0   Green       3.1  Apple\n",
       "1     Red       3.2  Apple\n",
       "2     Red       1.2  Grape\n",
       "3     Red       1.0  Grape\n",
       "4  Yellow       3.3  Lemon"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = [\n",
    "    ['Green', 3.1, 'Apple'],\n",
    "    ['Red', 3.2, 'Apple'],\n",
    "    ['Red', 1.2, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3.3, 'Lemon'],\n",
    "    ['Yellow', 3.1, 'Lemon'],\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Red', 1.1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "    ['Red', 1.2, 'Grape'],\n",
    "]\n",
    "\n",
    "header = [\"color\", \"diameter\", \"label\"]\n",
    "\n",
    "df = pd.DataFrame(data=training_data, columns=header)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gini impurity\n",
    "def calculate_gini(data):\n",
    "    \n",
    "    #取的資料的label訊息\n",
    "    label_column = data[:, -1]\n",
    "    \n",
    "    #取得所有輸入資料的獨立類別與其個數\n",
    "    _,counts = np.unique(label_column, return_counts=True)\n",
    "    \n",
    "    #計算機率\n",
    "    probabilities = counts / counts.sum()\n",
    "    \n",
    "    #計算gini impurity\n",
    "    gini = 1 - sum(probabilities**2)\n",
    "    \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'diameter <= 1.2': ['Grape', {'color = Yellow': ['Lemon', 'Apple']}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分割資料集\n",
    "train_df, test_df = train_test_split(df, 0.2)\n",
    "\n",
    "#以Gini inpurity作為metric_function訓練決策樹\n",
    "tree = decision_tree(calculate_gini, 'classification', 0, min_samples=2, max_depth=5)\n",
    "tree.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lemon'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以建構好的樹進行預測\n",
    "sample = test_df.iloc[0]\n",
    "tree.pred(sample, tree.sub_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "color       Yellow\n",
       "diameter       3.1\n",
       "label        Lemon\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#實作隨機森林\n",
    "class random_forest():\n",
    "    '''Random forest model\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_boostrap: int\n",
    "        number of samples to sample to train indivisual decision tree\n",
    "    n_tree: int\n",
    "        number of trees to form a forest\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_bootstrap, n_trees, task_type, min_samples, max_depth, metric_function, n_features=None):\n",
    "        self.n_bootstrap = n_bootstrap\n",
    "        self.n_trees = n_trees\n",
    "        self.task_type = task_type\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.metric_function = metric_function\n",
    "        self.n_features = n_features\n",
    "    \n",
    "    def bootstrapping(self, train_df, n_bootstrap):\n",
    "        #sample data to be used to train individual tree\n",
    "        bootstrap_indices = np.random.randint(low=0, high=len(train_df), size=n_bootstrap)\n",
    "        df_bootstrapped = train_df.iloc[bootstrap_indices]\n",
    "        \n",
    "        #avoid pick the samples with all the same label\n",
    "        while len(df_bootstrapped['label'].unique()) == 1:\n",
    "            bootstrap_indices = np.random.randint(low=0, high=len(train_df), size=n_bootstrap)\n",
    "            df_bootstrapped = train_df.iloc[bootstrap_indices]\n",
    "        \n",
    "        return df_bootstrapped\n",
    "    \n",
    "    def fit(self, train_df):\n",
    "        \n",
    "        self.forest = []\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            df_bootstrapped = self.bootstrapping(train_df, self.n_bootstrap)\n",
    "            tree = decision_tree(self.metric_function, self.task_type,\n",
    "                                 0, self.min_samples, self.max_depth,\n",
    "                                 self.n_features)\n",
    "            \n",
    "            tree.fit(df_bootstrapped)\n",
    "            self.forest.append(tree)\n",
    "            \n",
    "        return self.forest\n",
    "    \n",
    "    def pred(self, test_df):\n",
    "        df_predictions = {}\n",
    "        \n",
    "        for i in range(len(self.forest)):\n",
    "            # get prediction of every trees\n",
    "            col_name = f\"tree_{i}\"\n",
    "            predictions = list(test_df.apply(self.forest[i].pred, args=(self.forest[i].sub_tree,), axis=1))\n",
    "            df_predictions[col_name] = predictions\n",
    "            \n",
    "        df_predictions = pd.DataFrame(df_predictions)\n",
    "        \n",
    "        #majority voting\n",
    "        #mode: 最常出現的值\n",
    "        #axis = 1 獲取每一行的mode\n",
    "        random_forest_predictions = df_predictions.mode(axis=1)[0]\n",
    "        \n",
    "        return random_forest_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.decision_tree at 0x1a48777c198>,\n",
       " <__main__.decision_tree at 0x1a48662b0b8>,\n",
       " <__main__.decision_tree at 0x1a4845f82e8>,\n",
       " <__main__.decision_tree at 0x1a486aa4080>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, 0.2)\n",
    "\n",
    "#建立隨機森林模型\n",
    "forest = random_forest(n_bootstrap=20, n_trees=4, task_type='classification',\n",
    "                       min_samples=2, max_depth=5, metric_function=calculate_gini,\n",
    "                       n_features=None)\n",
    "\n",
    "forest.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Apple\n",
       "1    Apple\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.pred(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>diameter</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Lemon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    color  diameter  label\n",
       "8  Yellow       3.0  Lemon\n",
       "5  Yellow       3.1  Lemon"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
