{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day11 Part of Search Tagging.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "404pbUmSoX8l",
        "outputId": "c4551909-5aa5-4683-9c5d-47d371629775"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmT9PauUofJR",
        "outputId": "3b80b2b5-98a6-41d6-bc5c-9f992c1ba7dd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = 'Wow... Loved this place.'\n",
        "tokenize = nltk.word_tokenize(sentence)\n",
        "print('Token: {}'.format('|'.join(tokenize)))\n",
        "tagging = nltk.pos_tag(tokenize,)\n",
        "print(tagging)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Token: Wow|...|Loved|this|place|.\n",
            "[('Wow', 'NNP'), ('...', ':'), ('Loved', 'VBD'), ('this', 'DT'), ('place', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4zIGJINsWs3",
        "outputId": "7eed078d-8fb7-4b55-81b1-e7b107454f4e"
      },
      "source": [
        "import jieba\n",
        "import jieba.posseg as pseg\n",
        "sentence = '''這是敘述近年來，AI 應用已無所不在，不論在新創或是傳產領域，都可能透過機器學習解決過去難以解決的問題。但目前台灣企業在 AI 導入的腳步仍然緩慢，除了人才嚴重短缺，教育資源無法即時跟上產業變異也是原因之一。因此，我們發起了「機器學習百日馬拉松」教練陪跑計劃，翻轉傳統上課模式，以自主練習為主，幫助你獲得最大學習成效，搶先一步進入 AI 人工智能領域。'''\n",
        "word_list = [\"機器學習\",\"無所不在\",\"台灣\",\"企業\",\"腳步\",\"嚴重\",\"解決\",\"新創\",\"傳產\",\"領域\",\"過去\",\"搶先\",\"無法\",\"即時\",\"問題\",\"敘述\",\"應用\",\"我們\"\n",
        "      ,\"難以\",\"導入\",\"緩慢\",\"資源\",\"產業\",\"變異\",\"產業\",\"發起\",\"馬拉松\",\"教練\",\"翻轉\",\"上課\",\"練習\",\"幫助\",\"學習\",\"計劃\",\"不論\",\"透過\"]\n",
        "for word in word_list:\n",
        "  jieba.add_word(word)\n",
        "\n",
        "print(\"output 精確模式: {}\".format('|'.join(jieba.cut(sentence, cut_all=False, HMM=False))))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output 精確模式: 這|是|敘述|近年|來|，|AI| |應用|已|無所不在|，|不論|在|新創|或是|傳產|領域|，|都|可能|透過|機器學習|解決|過去|難以|解決|的|問題|。|但|目前|台灣|企業|在| |AI| |導入|的|腳步|仍然|緩慢|，|除了|人才|嚴重|短缺|，|教育|資源|無法|即時|跟上|產業|變異|也|是|原因|之一|。|因此|，|我們|發起|了|「|機器學習|百日|馬拉松|」|教練|陪|跑|計劃|，|翻轉|傳|統|上課|模式|，|以|自主|練習|為|主|，|幫助|你|獲|得|最大|學習|成效|，|搶先|一步|進|入| |AI| |人工智能|領域|。\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8wwwhxPPqHn",
        "outputId": "cf1e07b9-56ac-47b3-a7b7-2f08b060e4f5"
      },
      "source": [
        "words = pseg.cut(sentence,)\n",
        "for word, flag in words:\n",
        "    print(word, flag)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "這 r\n",
            "是 v\n",
            "敘述 x\n",
            "近年 t\n",
            "來 zg\n",
            "， x\n",
            "AI eng\n",
            "  x\n",
            "應用 x\n",
            "已 d\n",
            "無所不在 x\n",
            "， x\n",
            "不論 x\n",
            "在 p\n",
            "新創 x\n",
            "或是 c\n",
            "傳產 x\n",
            "領域 x\n",
            "， x\n",
            "都 d\n",
            "可能 v\n",
            "透過 x\n",
            "機器學習 x\n",
            "解決 x\n",
            "過去 x\n",
            "難以 x\n",
            "解決 x\n",
            "的 uj\n",
            "問題 x\n",
            "。 x\n",
            "但 c\n",
            "目前 t\n",
            "台灣 x\n",
            "企業 x\n",
            "在 p\n",
            "  x\n",
            "AI eng\n",
            "  x\n",
            "導入 x\n",
            "的 uj\n",
            "腳步 x\n",
            "仍然 d\n",
            "緩慢 x\n",
            "， x\n",
            "除了 p\n",
            "人才 n\n",
            "嚴重 x\n",
            "短缺 a\n",
            "， x\n",
            "教育 vn\n",
            "資源 x\n",
            "無法 x\n",
            "即時 x\n",
            "跟上 f\n",
            "產業 x\n",
            "變異 x\n",
            "也 d\n",
            "是 v\n",
            "原因 n\n",
            "之一 r\n",
            "。 x\n",
            "因此 c\n",
            "， x\n",
            "我們 x\n",
            "發起 x\n",
            "了 ul\n",
            "「 x\n",
            "機器學習 x\n",
            "百日 m\n",
            "馬拉松 x\n",
            "」 x\n",
            "教練 x\n",
            "陪 v\n",
            "跑 v\n",
            "計劃 x\n",
            "， x\n",
            "翻轉 x\n",
            "傳統 n\n",
            "上課 x\n",
            "模式 n\n",
            "， x\n",
            "以 p\n",
            "自主 v\n",
            "練習 x\n",
            "為 p\n",
            "主 b\n",
            "， x\n",
            "幫助 x\n",
            "你 r\n",
            "獲得 v\n",
            "最大 a\n",
            "學習 x\n",
            "成效 a\n",
            "， x\n",
            "搶先 x\n",
            "一步 m\n",
            "進入 v\n",
            "  x\n",
            "AI eng\n",
            "  x\n",
            "人工智能 n\n",
            "領域 x\n",
            "。 x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClboQQcuRqFY",
        "outputId": "542c1613-0960-4557-d7f6-c38d9684070e"
      },
      "source": [
        "!pip install sklearn_crfsuite"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn_crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.7)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.41.1)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 5.7MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIHZAOVwRneI"
      },
      "source": [
        "##資料來源：https://medium.com/analytics-vidhya/pos-tagging-using-conditional-random-fields-92077e5eaa31\n",
        "#pip install nltk\n",
        "import nltk\n",
        "#nltk.download()\n",
        "import re\n",
        "#pip install sklearn_crfsuite\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "#pip install scikit-learn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOzyZ38LRzBh",
        "outputId": "50d6333f-8c3f-4426-f1aa-523eb2dba663"
      },
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
        "print(\"Number of Tagged Sentences \",len(tagged_sentence))\n",
        "tagged_words=[tup for sent in tagged_sentence for tup in sent]\n",
        "print(\"Total Number of Tagged words\", len(tagged_words))\n",
        "vocab=set([word for word,tag in tagged_words])\n",
        "print(\"Vocabulary of the Corpus\",len(vocab))\n",
        "tags=set([tag for word,tag in tagged_words])\n",
        "print(\"Number of Tags in the Corpus \",len(tags))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "Number of Tagged Sentences  3914\n",
            "Total Number of Tagged words 100676\n",
            "Vocabulary of the Corpus 12408\n",
            "Number of Tags in the Corpus  12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJWtDKPaUWiA",
        "outputId": "c97820fd-a4c5-4e4c-f604-6b8deafee671"
      },
      "source": [
        "train_set, test_set = train_test_split(tagged_sentence,test_size=0.2,random_state=1234)\n",
        "print(\"Number of Sentences in Training Data \",len(train_set))\n",
        "print(\"Number of Sentences in Testing Data \",len(test_set))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Sentences in Training Data  3131\n",
            "Number of Sentences in Testing Data  783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzc9VX3ZVBS1"
      },
      "source": [
        "import re\n",
        "def features(sentence,index):\n",
        "    ### sentence is of the form [w1,w2,w3,..], index is the position of the word in the sentence\n",
        "    return {\n",
        "        'is_first_capital':int(sentence[index][0].isupper()),\n",
        "        'is_first_word': int(index==0),\n",
        "        'is_last_word':int(index==len(sentence)-1),\n",
        "        'is_complete_capital': int(sentence[index].upper()==sentence[index]),\n",
        "        'prev_word':'' if index==0 else sentence[index-1],\n",
        "        'next_word':'' if index==len(sentence)-1 else sentence[index+1],\n",
        "        'is_numeric':int(sentence[index].isdigit()),\n",
        "        'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
        "        'prefix_1':sentence[index][0],\n",
        "        'prefix_2': sentence[index][:2],\n",
        "        'prefix_3':sentence[index][:3],\n",
        "        'prefix_4':sentence[index][:4],\n",
        "        'suffix_1':sentence[index][-1],\n",
        "        'suffix_2':sentence[index][-2:],\n",
        "        'suffix_3':sentence[index][-3:],\n",
        "        'suffix_4':sentence[index][-4:],\n",
        "        'word_has_hyphen': 1 if '-' in sentence[index] else 0  \n",
        "         }\n",
        "def untag(sentence):\n",
        "    return [word for word,tag in sentence]\n",
        "\n",
        "\n",
        "def prepareData(tagged_sentences):\n",
        "    X,y=[],[]\n",
        "    for sentences in tagged_sentences:\n",
        "        X.append([features(untag(sentences), index) for index in range(len(sentences))])\n",
        "        y.append([tag for word,tag in sentences])\n",
        "    return X,y"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpbELGbtV23g"
      },
      "source": [
        "X_train,y_train=prepareData(train_set)\n",
        "X_test,y_test=prepareData(test_set)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnTUgLNkV6g8",
        "outputId": "f672ebc1-f9b4-4365-d795-8ea16b1b8797"
      },
      "source": [
        "crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=True,\n",
              "    averaging=None, c=None, c1=0.01, c2=0.1, calibration_candidates=None,\n",
              "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
              "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
              "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
              "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
              "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1GkzzuNWYvC",
        "outputId": "44d75625-4433-406c-ccd8-bf362c032cdf"
      },
      "source": [
        "y_pred=crf.predict(X_test)\n",
        "print(\"F1 score on Test Data \")\n",
        "print(metrics.flat_f1_score(y_test, y_pred,average='weighted',labels=crf.classes_))\n",
        "print(\"F score on Training Data \")\n",
        "y_pred_train=crf.predict(X_train)\n",
        "metrics.flat_f1_score(y_train, y_pred_train,average='weighted',labels=crf.classes_)\n",
        "\n",
        "### Look at class wise score\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=crf.classes_, digits=3\n",
        "))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on Test Data \n",
            "0.9738471726864286\n",
            "F score on Training Data \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADP      0.979     0.985     0.982      1869\n",
            "        NOUN      0.966     0.977     0.972      5606\n",
            "        CONJ      0.994     0.994     0.994       480\n",
            "        VERB      0.964     0.960     0.962      2722\n",
            "         ADJ      0.911     0.874     0.892      1274\n",
            "           .      1.000     1.000     1.000      2354\n",
            "           X      1.000     0.997     0.998      1278\n",
            "         NUM      0.991     0.993     0.992       671\n",
            "         DET      0.994     0.995     0.994      1695\n",
            "         ADV      0.927     0.909     0.918       585\n",
            "        PRON      0.998     0.998     0.998       562\n",
            "         PRT      0.979     0.982     0.980       614\n",
            "\n",
            "    accuracy                          0.974     19710\n",
            "   macro avg      0.975     0.972     0.974     19710\n",
            "weighted avg      0.974     0.974     0.974     19710\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}