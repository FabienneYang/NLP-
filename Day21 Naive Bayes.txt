Day21 Naive Bayes
Q: 為什麼特徵獨立的假設足以讓 Naive Bayes 被稱為 Naive?
A: NBC模型假設屬性之間相互獨立，這個假設在實際應用中往往是不成立的

Reference: https://towardsdatascience.com/whats-so-naive-about-naive-bayes-58166a6a9eba

Day25 Random Forest
Q: 一般模型誤差可拆為三部分，這三部分的誤差分別是？
A: 整體誤差(Total error)= 偏差(Bias) + 變異(variance) + 隨機誤差(random error)

Q: 解釋何謂Bias-Variance Tradeoff
A: 透過權衡 Bias Error 跟 Variance Error 來使得總誤差( Total Error ) 達到最小
   隨著模型複雜度的增加偏差會越來越低；而方差卻呈現了越來越高的趨勢，兩者是呈現 Tradeoff 的，只有在模型複雜度適中的時候，才有辦法得到最低的總誤差
   
Reference: https://jason-chen-1992.weebly.com/home/-bias-variance-tradeoff

Day26
Q: 描述Blending與Stacking的差異
A: Blending與Stacking都會產生2nd level的meta model，但其所使用來訓練meta model的方法不同。 
   Stacking使用cross validation來產生訓練2nd level model所需要的訓練資料
   Blending是在一開始就將training data切出一部分(holdout)來作為2nd level model的訓練資料
